---
layout: post
comments: true
title: Speed up Autonomous Driving with Safe Reinforcement Learning in Metadrive
author: Yanxun Li & Zixian Li (Team 19)
date: 2022-10-15
---

> In MetaDrive, safety is the first priority. We want to explore how to speed up auto-drive agents with Safe Reinforcement Learning and find the optimal speed in diverse driving scenarios.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Abstract
 Reinforcement Learning (RL) is widely used in safe autonomous driving. MetaDrive [1] is a driving simulation platform that composes diverse driving scenarios for generalizable RL. In MetaDrive, safety is the first priority, where agents were trained to ensure the driving safety in diverse driving scenarios. However, safety is overemphasized at the expense of speed. For example, when there is traffic, an agent may keep the speed as low as possible and not overtake other vehicles. Imagine there is a busy driver who wants to pass through the traffic flow and arrive the destination in the shortest amount of time with safe driving. How can we achieve that in MetaDrive? So we are going to explore how RL can have agents achieve fastest speed with safety guaranteed.

## Objective
Explore how to speed up auto-drive agents with safe RL and find the optimal speed in diverse driving scenarios.

## Potential Approach
Under the MetaDrive environment, we will look into the reward function and adjust the reward for speed. The new reward function will reward more for higher speed than the old one. However, higher speed leads to lower safety. So we need to experiment to find out the balance between speed and safety.

In terms of maximizing speed, [2] and [3] used deep RL to optimaze car racing. Although speeding up our agents is far from racing, we can research into the two papers and leverage deep RL to optimize speed in our case.

With speed being considered as priority, the definition of safety needs to be changed slightly. In terms of safety, our goal is to avoid collisions between vehicles and achieve safe driving. For example, when there is no traffic light in the crossroad, the vehicle should not make dangerous decision to disturb other vehicles. There are two methods to achieve: 
- Increase the safety distance between vehicle and complete turning to its goal direction in a specific times.
- Minimize the disturbance to other vehicles by changing to other lanes with less cars and complete turning to its goal direction in a specific times.

We found [4] and [5] are helpful to reference in developing safe RL methods. We will firstly generate a few scenarios to simulate different traffic flows in MetaDrive from relatively empty road cases to busy road cases. Then a single agent will be designed and trained in those scenarios. We will focus on researching small sized vehicles and design the reward function mentioned above, where speed and risk factor are primary. After that, the designed agent could be trained by calculating its own reward on each state and update the state-value and policy using dynamic programming based on Markov decision process. Besides, we will compare both the policy iteration as well as the value iteration to see which will converges faster and which will generate the safest and fastest trajectary on the road.Chaning lane and overtaking is permitted to achieve faster speed.


## Potential Environment
MetaDrive in Python

## Midterm Progress
We have successfully set up the environment for MetaDrive. We dived into the source code of MetaDrive and figured out the architecture of MetaDrive e.g. the MataDrive Environment controlls agent vehicle with pre-trained PPO policy. 

We updated reward function in order to obtain an agent with faster speed and relatively high safety.

$$
R=c_1R_{driving}+c_2R_{speed}+R_{termination}
$$

We increased the maximum speed limitation of vehicle. At the same time, we increased the speed reward factor $$c_2$$ to emphasize the importance of speed, and thus the vehicle will be encouraged to take more acceleration as long as the safety factor is satisfied. Moreover, we set "use_lateral_reward" to false so that the vehicle will not keep in lane and learn to overtake cars and achieve faster speed. The newly updated reward function is used to in training process to generate the policy function.

We found MetaDrive uses RLLib to train RL agents. Ray 2.1.0 in RLLib is used to train and generate policy parameters with the PPO reinforcement training method under 22 environments for a large amount of episode, and the parameters are saved in the local folder. We also found MetaDrive uses expert class to read and make use of the trained data (e.g. weight), which returns the vehicle's current observation and continuous action to AI policy function. The trained policy function fits the fast and safe targets in the environment.

#### Next Steps: ####

We are going to research on how to run the training with RLLib and train the agent with the new reward function to see the result. We believe that the new reward function might not be sufficient to perfectly speed up the AI agent with safety, but it will serve as a good indication of what steps we need to take next e.g. change more parameters or change to a different RL method rather than PPO.


##Experiment and result
###objective:
Baseline: our baseline is to keep safety with IPPO algorithm in roundabout environment,which means we hope the success rate of agents will not change. Therefore, we will use success rate as our metric to judge the performance of experiments.The baseline here are the training bestcheck points provided by the author.
Goal: In the same time, our goal is speed up the whole environment running process while keep the same safety.Therefore, the total running time of all agents will become our metric to judge the performance of experiments.

###Experiment process
Comtrolling variable method would be used to make accurate comparison of results. In order to achieve our objective,the experiment includes 5 steps for each environment basically:
1.Use the methods and functions provided by Rllib and Tune, which is a python package for reinforcemnet agent training.After that,we could train the ippo trainer in different environments with 1,000,000 steps. ippo is an algorithm called independent PPO used for multi-agent reinforcement learning. Besides, the environment will be focus in roundabout environment with different training parameter.
2.Use the best checkpoint during training process with the highest success rate. Since the training agent may overfit when the training step is two large, we need to evaluate and find the best checkpoint during the training process and return a policy function.
3.In order to obtain the action in specific situation, observation and done function (o,d) should be input into the policy function.
4.The action generated by policy function could be given to the simulation environment and step into the next state.
5.The total running time and success rate could be calculated for each episode when all agents terminate, which could be crash, out of road or reach destination.The data could be collected and analyzed.

###Use lateral reward in training
parameter:lateral reward=True 
env:roundabout
algorithm:ippo

####Intuition: 
Observed that in the official baseline of ippo in roundabout environment, the vehicles always drive in the middle of the road which stop the vehicle behind from speeding up or overtaking. At the same time,this trained vehicles prefer to drive in the outside lane of the road and not to takeover the car because of safety consideration although without the use of lateral reward. We think this is a kind of waste of traffic resources since the inside lane is always empty while this may also be one reason leads the slow speed of vehicle. Therefore,with the use of lateral reward, the car would learn to keep in the lane, so that the traffic flow could be divided into 2 lanes and use the traffic resources sufficiently. And the car may be able to speed up since there are less car in one lane.

####Training result:
Compared to the baseline, it could be found that the car has been divided into 2 lanes so the traffic resource are utilized better. Therefore, the total running time have improvement compared to the baseline. However, due to the reason that the outside vehicles and inside vehicle are easy to drive in the same pace, it is much easier to get crashed. Therefore, the success rate declines slightly.

放两个的gif和与baseline的表格对比图

###Increase speed reward in training




##Comparison and Discussion


## Relevant Papers
[1] Li, Quanyi, et al. "MetaDrive: Composing diverse driving scenarios for generalizable reinforcement learning." IEEE transactions on pattern analysis and machine intelligence (2022).

[2] Aldape, Pablo, and Samuel Sowell. "Reinforcement Learning for a Simple Racing Game." Stanford, 2018.

[3] Remonda, Adrian, et al. "Formula RL: Deep Reinforcement Learning for Autonomous Racing using Telemetry Data." CoRR, abs/2104.11106, 2021.

[4] D. Isele, A. Nakhaei and K. Fujimura, "Safe Reinforcement Learning on Autonomous Vehicles," 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018, pp. 1-6, doi: 10.1109/IROS.2018.8593420.

[5] Berkenkamp F, Turchetta M, Schoellig A, et al. Safe model-based reinforcement learning with stability guarantees[J]. Advances in neural information processing systems, 2017, 30.
